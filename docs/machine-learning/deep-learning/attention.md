# Attention is All You Need

最近 Attention 出现频率已经非常高了，也是有必要去学习一下的。学习过后发现其实，可能未来真的是 `Attention is All You Need` 了。

大部分学习的内容来自李宏毅老师的视频课程，见 Reference 3,4。

## 写在前面

其实注意力机制只是整个神经网络中的一个 Module，并且有着异常多的变种，像残差网络一样，每个方法其实都不复杂三言两语就能介绍完，但是想要深入理解他的设计和实现之类的原理性问题，其实还是非常奇妙的。

## 自注意力机制

可能谈起自注意力机制，就要从 《Attention is All You Need》 开始，虽然他并不是第一个提出 Attention 这类概念的，但确实是他开始把 Attention 发扬光大的，包括后面的 Transformer 和 Bert。

# Reference

1. [Attention is All You Need](https://arxiv.org/abs/1706.03762)
2. [《Attention is All You Need》浅读（简介+代码）](https://kexue.fm/archives/4765) - 苏剑林
3. [【機器學習 2021】自注意力機制 (Self-attention) (上)](https://www.youtube.com/watch?v=hYdO9CscNes) - 李宏毅
4. [【機器學習 2021】自注意力機制 (Self-attention) (下)](https://www.youtube.com/watch?v=gmsMY5kc-zw) - 李宏毅
